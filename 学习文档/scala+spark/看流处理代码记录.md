#4/15号看流处理代码记录

流处理代码主要采用Scala编程，
疑问1：为何代码中是以Object开始的?
答：java中是以class开始，scala是以object开始，在Scala中有伴生类的概念，其实就是一个意思，为了防止告抄袭？

2.    val inputDstreamMeteData = KafkaUtils.createDirectStream[String, String](
      ssc,
      PreferConsistent,
      Subscribe[String, String](Array(topic), kafkaParameter)
    )

Spark Streaming内部的基本工作原理：接收实时输入数据流，然后将数据拆分成多个batch，比如每收集1s的数据封装为一个batch， 
然后将每个batch交给Spark的计算引擎进行处理，最后会生产出一个结果数据流，其中的数据，也是一个个的batch所组成的。
其中，一个batchInterval累加读取到的数据对应一个RDD的数据




checkpoint：用来保存和恢复，一般需要设置目录进行保存
DStream https://www.jianshu.com/p/931651943104
DStream：Discretized Stream，离散流，Spark Streaming提供的一种高级抽象，代表了一个持续不断的数据流；
DStream的内部，其实是一系列持续不断产生的RDD，RDD是Spark Core的核心抽象，即，不可变的，分布式的数据集；就是一个装数据的集合
DStream中的每个RDD都包含了一个时间段内的数据；（可以设置一个RDD装多少个batch）


一个StreamingContext定义之后，必须做以下几件事情：

1、通过创建输入DStream来创建输入数据源。
2、通过对DStream定义transformation和output算子操作，来定义实时计算逻辑。
3、调用StreamingContext的start()方法，来开始实时处理数据。
4、调用StreamingContext的awaitTermination()方法，来等待应用程序的终止。可以使用CTRL+C手动停止，或者就是让它持续不断的运行进行计算。
5、也可以通过调用StreamingContext的stop()方法，来停止应用程序。


2、基于Direct的方式
基于Direct的方式，是在Spark 1.3中引入的，能够确保更加健壮的机制。
替代掉使用Receiver来接收数据后，这种方式会周期性地查询Kafka，来获得每个topic+partition的最新的offset，从而定义每个batch的offset的范围。
当处理数据的job启动时，就会使用Kafka的简单consumer api来获取Kafka指定offset范围的数据。

这种方式有如下优点：
1、简化并行读取：如果要读取多个partition，不需要创建多个输入DStream然后对它们进行union操作；
Spark会创建跟Kafka partition一样多的RDD partition，并且会并行从Kafka中读取数据，所以在Kafka partition和RDD partition之间，有一个一对一的映射关系。

2、高性能：如果要保证零数据丢失，在基于receiver的方式中，需要开启WAL机制。这种方式其实效率低下，因为数据实际上被复制了两份，Kafka自己本身就有高可靠的机制，会对数据复制一份，而这里又会复制一份到WAL中。而基于direct的方式，不依赖Receiver，不需要开启WAL机制，只要Kafka中作了数据的复制，那么就可以通过Kafka的副本进行恢复。

3、一次且仅一次的事务机制：
基于receiver的方式，是使用Kafka的高阶API来在ZooKeeper中保存消费过的offset的，这是消费Kafka数据的传统方式。
这种方式配合着WAL机制可以保证数据零丢失的高可靠性，但是却无法保证数据被处理一次且仅一次，可能会处理两次。
因为Spark和ZooKeeper之间可能是不同步的。

而基于direct的方式，使用kafka的简单api，Spark Streaming自己就负责追踪消费的offset，并保存在checkpoint中；
Spark自己一定是同步的，因此可以保证数据是消费一次且仅消费一次。

作者：张凯_9908
链接：https://www.jianshu.com/p/23b1c71e45f6
来源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。